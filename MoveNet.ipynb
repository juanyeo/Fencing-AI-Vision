{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d006a55",
   "metadata": {},
   "source": [
    "# Fencing Computer Vision Model Test #1\n",
    "\n",
    "## MoveNet - Multi Person Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636c46c",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee28b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a381650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import movenet_multipose_lightning_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3ed8f",
   "metadata": {},
   "source": [
    "# Load MoveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb1dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 10:57:54.819004: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Download the model from TF Hub.\n",
    "#model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "#model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "model = hub.load(\"/Users/juan/Desktop/Python/fencingai/movenet_multipose_lightning_1/\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26ff07",
   "metadata": {},
   "source": [
    "## draw points and lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(frame, person, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(person, [y, x, 1]))\n",
    "    \n",
    "    for p in shaped:\n",
    "        y, x, conf = p\n",
    "        if conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(x), int(y)), 4, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb444f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def draw_connections(frame, person, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(person, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816c9cd",
   "metadata": {},
   "source": [
    "## Multi Person Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9131d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_people(frame, points, edges, confidence_threshold):\n",
    "    for person in points:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_points(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c588cc",
   "metadata": {},
   "source": [
    "# Pose Detection (webcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Model Requirement: Resize 256 x 256 (multiple of 32), int32 Type\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 160, 256)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Make Detection\n",
    "    results = movenet(input_img)\n",
    "    points = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    # Draw Pose\n",
    "    loop_through_people(frame, points, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow('MoveNet', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f94ba",
   "metadata": {},
   "source": [
    "### Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d01cf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.4123420e-01, 2.7708939e-01, 6.9145709e-01, 4.0667498e-01,\n",
       "       3.2739532e-01, 4.9196729e-01, 4.1070819e-01, 2.5615749e-01,\n",
       "       8.0806792e-01, 4.4805995e-01, 4.1284192e-01, 6.7486602e-01,\n",
       "       4.4569373e-01, 2.4323143e-01, 7.4638480e-01, 6.9111764e-01,\n",
       "       4.9377954e-01, 8.1511635e-01, 6.6011971e-01, 1.5455548e-01,\n",
       "       6.8172604e-01, 7.8068036e-01, 5.0935858e-01, 8.8405922e-02,\n",
       "       7.7705544e-01, 6.6064045e-02, 1.0612572e-01, 7.5486088e-01,\n",
       "       4.1013539e-01, 3.4292463e-02, 7.7671880e-01, 1.4037894e-01,\n",
       "       2.1936417e-01, 7.8182852e-01, 4.0292582e-01, 3.7871185e-05,\n",
       "       7.8166556e-01, 1.7132211e-01, 3.2979000e-04, 7.5815898e-01,\n",
       "       3.5965768e-01, 2.7594257e-03, 7.6537937e-01, 7.0583954e-02,\n",
       "       3.3933967e-02, 4.6543241e-01, 4.1075364e-01, 3.3177828e-04,\n",
       "       4.3542874e-01, 1.3696021e-01, 6.9100811e-04, 2.7762896e-01,\n",
       "       1.6489744e-02, 7.7694255e-01, 5.6312305e-01, 6.2338620e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['output_0'].numpy()[:,:,:56][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a9bc3",
   "metadata": {},
   "source": [
    "# Pose Detection (video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c44a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('fenc_video_1.mov')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Model Requirement: Resize 256 x 256 (multiple of 32), int32 Type\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 160, 256)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Make Detection\n",
    "    results = movenet(input_img)\n",
    "    points = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    # Draw Pose\n",
    "    loop_through_people(frame, points, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow('MoveNet', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a25a0",
   "metadata": {},
   "source": [
    "# Pose Detection (forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b65c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('fenc_video_2.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Model Requirement: Resize 256 x 256 (multiple of 32), int32 Type\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 256, 256)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Make Detection\n",
    "    results = movenet(input_img)\n",
    "    points = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    # Draw Pose\n",
    "    loop_through_people(frame, points, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow('MoveNet', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aacc84",
   "metadata": {},
   "source": [
    "# Pose Detection (Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af3dbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('fenc_video_3.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Model Requirement: Resize 256 x 256 (multiple of 32), int32 Type\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 128, 256)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Make Detection\n",
    "    results = movenet(input_img)\n",
    "    points = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    # Draw Pose\n",
    "    loop_through_people(frame, points, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow('MoveNet', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5045e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('fenc_video_4.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Model Requirement: Resize 256 x 256 (multiple of 32), int32 Type\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 256, 256)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Make Detection\n",
    "    results = movenet(input_img)\n",
    "    points = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    # Draw Pose\n",
    "    loop_through_people(frame, points, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow('MoveNet', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29df472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('fenc_video_5.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Model Requirement: Resize 256 x 256 (multiple of 32), int32 Type\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 192, 256)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Make Detection\n",
    "    results = movenet(input_img)\n",
    "    points = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    # Draw Pose\n",
    "    loop_through_people(frame, points, EDGES, 0.3)\n",
    "    \n",
    "    cv2.imshow('MoveNet', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1b615",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "영상을 256px 내외로 축소해서 detection을 하고 reshape를 하는 방식이어서 피사체가 작으면 정확도가 매우 떨어졌다. 카메라를 고정하고 펜싱 영상을 찍으면 피스트 전체를 담아야 해서 사람이 작게 나온다. 영상을 직접 찍어서 분석할 경우 MoveNet은 적합하지 않다고 생각된다.\n",
    "\n",
    "카메라가 선수를 줌인해서 움직이는 대회 영상을 분석할 경우 피사체가 크게 나와 사용이 가능할 것 같다. 다만 손목이 인식되지 않아 칼의 움직임을 추적할 수는 없다. 처리 시간이 빠르다는 장점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73389135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
